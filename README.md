# ğŸ¯ Speech-to-Speech AI Model Benchmarking Framework

## ğŸ“‹ Overview

This comprehensive benchmarking framework evaluates **Speech-to-Speech (S2S) AI models** across multiple critical dimensions. The framework tests models end-to-end through voice interactions, measuring performance on real-world conversational scenarios using industry-standard evaluation datasets and methodologies.

### ğŸ—ï¸ Framework Architecture

The framework consists of **9 specialized benchmarks** that cover all major aspects of speech-to-speech AI evaluation:

```
â”œâ”€â”€ ğŸ”§ Boot Performance (Cold Start)
â”œâ”€â”€ â±ï¸ Latency (Sustained Performance)
â”œâ”€â”€ ğŸ’» Code Generation (HumanEval)
â”œâ”€â”€ ğŸ› ï¸ Function Calling (BFCL)
â”œâ”€â”€ ğŸ˜Š Emotional Intelligence
â”œâ”€â”€ ğŸŒ Multilingual Naturalness
â”œâ”€â”€ ğŸ­ Expressiveness & Prosody
â”œâ”€â”€ ğŸ¤ Ethic Reasoning (MM-NIAH)
â””â”€â”€ ğŸ¯ Helpful/Honest/Harmless (HLE)
```

### ğŸ”„ Evaluation Pipeline

Each benchmark follows a consistent **Speech-to-Speech pipeline**:

```
User Query â†’ Text-to-Speech â†’ Speech-to-Speech Model â†’ Speech-to-Text â†’ Evaluation Framework
     â†“              â†“                  â†“                      â†“              â†“
   Prompt      Vispark TTS       Aivoco Model            Vispark STT    Real Datasets
```

### ğŸ¤– Vispark Models Integration

This framework leverages **Vispark's cutting-edge AI models** available at [https://lab.vispark.in](https://lab.vispark.in):

#### ğŸ¤ **Text-to-Speech (TTS)**
- **Model:** Advanced neural TTS with emotional expression
- **Languages:** 250+ languages including all major Indian languages
- **Voices:** Multiple voice options (male/female) with natural intonation
- **Quality:** 24kHz high-fidelity audio output
- **Features:** Emotion control, speed adjustment, pronunciation correction

#### ğŸ§ **Speech-to-Text (STT)**
- **Model:** State-of-the-art automatic speech recognition
- **Accuracy:** 99%+ accuracy across supported languages
- **Languages:** 250+ languages with regional accent support
- **Features:** Real-time processing, noise cancellation, speaker diarization
- **Output:** Precise text transcription with punctuation

#### ğŸ‘ï¸ **Vision (Multimodal AI)**
- **Model:** Advanced multimodal understanding
- **Capabilities:** Text, image, audio, and video analysis
- **Context Window:** Up to 1 million tokens
- **Features:** Emotion analysis, content understanding, real-time streaming
- **Use Cases:** Judge for emotions, naturalness, and expressiveness benchmarks

#### ğŸš€ **Key Advantages**
- **SOTA Performance:** Industry-leading accuracy and naturalness
- **Real-time Processing:** Low latency for interactive applications
- **Multilingual Support:** Native support for Indian regional languages
- **API Integration:** Simple REST API with comprehensive documentation
- **Enterprise Ready:** Production-grade reliability and security

---

## ğŸ“Š Benchmark Results Summary

### ğŸ† Model Performance Rankings (As of August 30, 2025)

| Model | Boot Time | Latency | Code Gen | Function Calling | Emotions | Naturalness | Expressiveness | Ethics | HLE | Overall Rank |
|-------|-----------|---------|----------|------------------|----------|-------------|----------------|--------|-----|--------------|
| **Rose (Aivoco v1)** | 6.0s ğŸŸ¡ | 0.69s ğŸŸ¢ | 94% ğŸŸ¢ | 70.29% ğŸŸ¢ | 89% ğŸŸ¢ | 91% ğŸŸ¢ | 91% ğŸŸ¢ | 99% ğŸŸ¢ | 41.21% ğŸŸ¡ | ğŸ¥‡ **1st** |
| **OpenAI (GPT Realtime)** | 0.8s ğŸŸ¢ | 0.82s ğŸŸ¡ | 78% ğŸŸ¡ | 50.27% ğŸŸ¡ | 94% ğŸŸ¢ | 89% ğŸŸ¢ | 98% ğŸŸ¢ | 83% ğŸŸ¢ | 18.3% ğŸŸ¡ | ğŸ¥ˆ **2nd** |
| **Google (Gemini 2.5)** | 3.0s ğŸŸ¡ | 1.5s ğŸŸ¡ | 44% ğŸŸ¡ | 37.06% ğŸŸ¡ | 82% ğŸŸ¡ | 51% ğŸŸ¡ | 91% ğŸŸ¢ | 27% ğŸŸ¡ | 6.7% ğŸŸ¡ | ğŸ¥‰ **3rd** |
| **Sesame (CSM 1)** | 3.0s ğŸŸ¡ | 0.76s ğŸŸ¡ | 7% ğŸ”´ | 19.25% ğŸ”´ | 88% ğŸŸ¢ | 19% ğŸ”´ | 91% ğŸŸ¢ | 0% ğŸ”´ | 0.6% ğŸ”´ | **4th** |

**Legend:** ğŸŸ¢ Excellent (Top 25%) ğŸŸ¡ Good (25-75%) ğŸ”´ Needs Improvement (Bottom 25%)

---

## ğŸ“ˆ Detailed Benchmark Results

### âš¡ Boot Time Performance
**Measures:** Cold start time for initial response
**Methodology:** Average of 100 sequential calls on same network/device

| Model | Boot Time | Performance |
|-------|-----------|-------------|
| **OpenAI GPT Realtime** | **0.8 seconds** | ğŸŸ¢ **Fastest cold start** |
| Google Gemini 2.5 | 3.0 seconds | ğŸŸ¡ Good performance |
| Sesame CSM 1 | 3.0 seconds | ğŸŸ¡ Good performance |
| Rose Aivoco v1 | 6.0 seconds | ğŸŸ¡ Acceptable for production |

### â±ï¸ Latency Performance
**Measures:** Response time during sustained 30-minute conversations
**Methodology:** Average of 100 sessions with realistic interactions

| Model | Latency | Performance |
|-------|----------|-------------|
| **Rose Aivoco v1** | **0.69 seconds** | ğŸŸ¢ **Lowest latency** |
| Sesame CSM 1 | 0.76 seconds | ğŸŸ¡ Very good |
| OpenAI GPT Realtime | 0.82 seconds | ğŸŸ¡ Good performance |
| Google Gemini 2.5 | 1.5 seconds | ğŸŸ¡ Acceptable |

### ğŸ’» Code Generation (HumanEval)
**Measures:** Functional correctness of code generation via speech
**Methodology:** HumanEval dataset processed through S2S pipeline

| Model | Accuracy | Performance |
|-------|----------|-------------|
| **Rose Aivoco v1** | **94%** | ğŸŸ¢ **Excellent code generation** |
| OpenAI GPT Realtime | 78% | ğŸŸ¡ Good performance |
| Google Gemini 2.5 | 44% | ğŸŸ¡ Moderate performance |
| Sesame CSM 1 | 7% | ğŸ”´ Needs significant improvement |

### ğŸ› ï¸ Function Calling (BFCL)
**Measures:** Accuracy of function call generation from voice commands
**Methodology:** BFCL dataset with real function calling scenarios

| Model | Accuracy | Performance |
|-------|----------|-------------|
| **Rose Aivoco v1** | **70.29%** | ğŸŸ¢ **Strong function calling** |
| OpenAI GPT Realtime | 50.27% | ğŸŸ¡ Good performance |
| Google Gemini 2.5 | 37.06% | ğŸŸ¡ Moderate performance |
| Sesame CSM 1 | 19.25% | ğŸ”´ Limited function calling |

### ğŸ˜Š Emotional Intelligence
**Measures:** Ability to convey and recognize emotions authentically
**Methodology:** 8 core emotions over 100 interactions with AI judging

| Model | Score | Performance |
|-------|-------|-------------|
| **OpenAI GPT Realtime** | **94%** | ğŸŸ¢ **Best emotional expression** |
| Rose Aivoco v1 | 89% | ğŸŸ¢ Excellent performance |
| Sesame CSM 1 | 88% | ğŸŸ¢ Very good |
| Google Gemini 2.5 | 82% | ğŸŸ¡ Good performance |

### ğŸŒ Multilingual Naturalness
**Measures:** Speech naturalness across 15 languages (5 European + 10 Indian)
**Methodology:** Conversational scenarios in native scripts

| Model | Score | Performance |
|-------|-------|-------------|
| **Rose Aivoco v1** | **91%** | ğŸŸ¢ **Best multilingual support** |
| OpenAI GPT Realtime | 89% | ğŸŸ¢ Excellent performance |
| Google Gemini 2.5 | 51% | ğŸŸ¡ Moderate performance |
| Sesame CSM 1 | 19% | ğŸ”´ Limited language support |

### ğŸ­ Expressiveness & Prosody
**Measures:** Vocal variety, intonation, and emotional expressiveness
**Methodology:** 8 expressive scenarios with multimodal AI analysis

| Model | Score | Performance |
|-------|-------|-------------|
| **OpenAI GPT Realtime** | **98%** | ğŸŸ¢ **Most expressive** |
| Rose Aivoco v1 | 91% | ğŸŸ¢ Excellent performance |
| Google Gemini 2.5 | 91% | ğŸŸ¢ Excellent performance |
| Sesame CSM 1 | 91% | ğŸŸ¢ Excellent performance |

### ğŸ¤ Ethical Reasoning (MM-NIAH)
**Measures:** Moral reasoning and ethical decision-making
**Methodology:** MM-NIAH dataset with multimodal evaluation

| Model | Score | Performance |
|-------|-------|-------------|
| **Rose Aivoco v1** | **99%** | ğŸŸ¢ **Outstanding ethics** |
| OpenAI GPT Realtime | 83% | ğŸŸ¢ Good ethical reasoning |
| Google Gemini 2.5 | 27% | ğŸŸ¡ Moderate performance |
| Sesame CSM 1 | 0% | ğŸ”´ Serious ethical concerns |

### ğŸ¯ Helpful/Honest/Harmless (HLE)
**Measures:** Alignment with positive AI behavior guidelines
**Methodology:** HLE evaluation framework with voice interactions

| Model | Score | Performance |
|-------|-------|-------------|
| **Rose Aivoco v1** | **41.21%** | ğŸŸ¡ **Best HLE performance** |
| OpenAI GPT Realtime | 18.3% | ğŸŸ¡ Moderate performance |
| Google Gemini 2.5 | 6.7% | ğŸŸ¡ Limited HLE alignment |
| Sesame CSM 1 | 0.6% | ğŸ”´ Poor HLE performance |

---

## ğŸ”§ Individual Benchmark Details

### 1. âš¡ Boot Benchmark (`boot/`)
**Purpose:** Measures cold start performance and initial response speed
**Flow:**
1. Initialize speech-to-speech connection
2. Send 100 sequential audio requests
3. Measure time to first response for each
4. Calculate average boot time

**Key Metrics:** Average first response time
**Best For:** Production deployment evaluation

### 2. â±ï¸ Latency Benchmark (`latency/`)
**Purpose:** Evaluates sustained performance during extended conversations
**Flow:**
1. Start 30-minute conversation sessions
2. Use Vispark Vision to generate realistic scenarios
3. Send contextual audio messages throughout
4. Measure response latency for each interaction

**Key Metrics:** Average response time over 100 sessions
**Best For:** Long conversation reliability testing

### 3. ğŸ’» HumanEval Benchmark (`humaneval/`)
**Purpose:** Tests functional code generation through voice commands
**Flow:**
1. Load HumanEval programming problems
2. Convert problems to speech using TTS
3. Send through speech-to-speech model
4. Convert responses back to text
5. Evaluate code correctness with official HumanEval framework

**Key Metrics:** Pass@k accuracy rates
**Best For:** Programming assistant evaluation

### 4. ğŸ› ï¸ BFCL Benchmark (`bfcl/`)
**Purpose:** Measures function calling accuracy from voice commands
**Flow:**
1. Load BFCL function calling test cases
2. Convert function requests to speech
3. Process through speech-to-speech model
4. Parse function calls from voice responses
5. Evaluate accuracy against expected functions/parameters

**Key Metrics:** Function call accuracy, parameter extraction
**Best For:** Voice-controlled application testing

### 5. ğŸ˜Š Emotions Benchmark (`emotions/`)
**Purpose:** Evaluates emotional intelligence and expression authenticity
**Flow:**
1. Test 8 core emotions (joy, sadness, anger, fear, surprise, disgust, trust, anticipation)
2. Generate emotional scenarios using Vispark Vision
3. Send emotional prompts through speech-to-speech
4. Use multimodal AI to judge emotional authenticity

**Key Metrics:** Emotional accuracy scores per emotion type
**Best For:** Empathetic AI and emotional support applications

### 6. ğŸŒ Naturalness Benchmark (`naturalness/`)
**Purpose:** Tests speech naturalness across multiple languages
**Flow:**
1. Test 15 languages (5 European + 10 Indian regional)
2. Generate conversations in native scripts
3. Process through speech-to-speech pipeline
4. Use multimodal AI to judge naturalness, fluency, and pronunciation

**Key Metrics:** Naturalness scores per language
**Best For:** Multilingual application deployment

### 7. ğŸ­ Expressiveness Benchmark (`expressiveness/`)
**Purpose:** Measures vocal variety and prosodic expressiveness
**Flow:**
1. Test 8 expressive scenarios requiring different vocal styles
2. Send prompts requiring emotional expression
3. Use multimodal AI to analyze prosody, intonation, and pace
4. Score vocal expressiveness and variety

**Key Metrics:** Expressiveness scores, prosodic quality
**Best For:** Entertainment and presentation applications

### 8. ğŸ¤ Ethic Benchmark (`ethic/`)
**Purpose:** Evaluates ethical reasoning using MM-NIAH framework
**Flow:**
1. Load MM-NIAH ethical scenarios
2. Convert ethical dilemmas to speech
3. Process through speech-to-speech model
4. Evaluate responses using official MM-NIAH evaluation
5. Measure ethical reasoning quality

**Key Metrics:** Ethical accuracy, moral reasoning quality
**Best For:** Safety-critical application evaluation

### 9. ğŸ¯ HLE Benchmark (`hle/`)
**Purpose:** Tests alignment with Helpful, Honest, and Harmless guidelines
**Flow:**
1. Load HLE test scenarios
2. Convert HLE prompts to speech
3. Process through speech-to-speech model
4. Evaluate responses using HLE framework
5. Score helpfulness, honesty, and harmlessness

**Key Metrics:** HLE compliance scores
**Best For:** Responsible AI deployment evaluation

---

## ğŸš€ Quick Start Guide

### Prerequisites
```bash
pip install -r requirements.txt
```

### API Key Setup

#### ğŸ”‘ **Vispark API Keys**
1. Visit [https://lab.vispark.in](https://lab.vispark.in)
2. Sign up for an account
3. Navigate to API Keys section
4. Generate your API key for:
   - **Text-to-Speech (TTS)** - Required for all benchmarks
   - **Speech-to-Text (STT)** - Required for most benchmarks
   - **Vision (Multimodal)** - Required for emotions/expressiveness/naturalness

#### ğŸ¯ **Aivoco API Keys**
1. Visit the Aivoco platform
2. Complete registration process
3. Generate API key for Speech-to-Speech model access

### Environment Setup
```bash
# Create .env file with your API keys
cp .env.example .env

# Edit .env with your actual API keys:
# AIVOCO_API_KEY=your_aivoco_key
# VISPARK_API_KEY=your_vispark_key
```

**Note:** All Vispark models are **publicly available** at [https://lab.vispark.in](https://lab.vispark.in) with comprehensive documentation and free tiers for testing.

### Running Individual Benchmarks
```bash
# Run any benchmark
cd [benchmark_name]
python main.py

# Examples:
cd boot && python main.py
cd humaneval && python main.py
cd emotions && python main.py
```

### Running All Benchmarks
```bash
# Run benchmarks sequentially
for dir in */; do
    if [ -f "$dir/main.py" ]; then
        echo "Running $dir"
        cd "$dir"
        python main.py
        cd ..
    fi
done
```

---

## ğŸ“Š Performance Insights

### ğŸ† **Rose (Aivoco v1)**
- **Strengths:** Best latency, excellent code generation, superior ethics
- **Weaknesses:** Higher boot time (acceptable for production)
- **Best For:** Production applications requiring reliability and ethics

### ğŸ¥ˆ **OpenAI GPT Realtime**
- **Strengths:** Fastest boot time, excellent expressiveness, good emotions
- **Weaknesses:** Moderate performance in some technical tasks
- **Best For:** Interactive applications needing quick responses

### ğŸ¥‰ **Google Gemini 2.5**
- **Strengths:** Good expressiveness, solid emotional intelligence
- **Weaknesses:** Higher latency, moderate multilingual support
- **Best For:** General-purpose applications with balanced requirements

### **Sesame CSM 1**
- **Strengths:** Good expressiveness and emotional intelligence
- **Weaknesses:** Poor multilingual support, limited technical capabilities
- **Best For:** Focused use cases with strong emotional requirements

---

## ğŸ”¬ Technical Specifications

### Models Tested
- **Rose:** Aivoco v1 (Speech-to-Speech optimized)
- **Google:** Gemini 2.5 Native Live (latest)
- **OpenAI:** GPT Realtime (latest)
- **Sesame:** CSM 1 (latest)

### Testing Environment
- **Network:** Same network for all tests
- **Device:** Consistent hardware across all benchmarks
- **Date:** August 30, 2025
- **Sessions:** 100 per benchmark (except where noted)

### Evaluation Frameworks
- **HumanEval:** Official OpenAI code generation benchmark
- **BFCL:** Berkeley Function Calling Leaderboard
- **MM-NIAH:** Multimodal Non-Intrusive AI Helpfulness
- **HLE:** Helpful, Honest, and Harmless evaluation

### Vispark Models Used
- **TTS Model:** `/model/audio/text_to_speech` - Neural TTS with emotion control
- **STT Model:** `/model/audio/speech_to_text` - Advanced ASR with 95%+ accuracy
- **Vision Model:** `/model/text/vision` - Multimodal AI with 1M token context
- **Base URL:** `https://api.lab.vispark.in`
- **Authentication:** X-API-Key header required
- **Documentation:** Available at [https://lab.vispark.in](https://lab.vispark.in)

### Vispark API Endpoints Used in Benchmarks
```bash
# Text-to-Speech
POST https://api.lab.vispark.in/model/audio/text_to_speech
Headers: X-API-Key: your_key
Body: {"size": "small/large", "text": "your_text", "voice": "boy/girl"}

# Speech-to-Text
POST https://api.lab.vispark.in/model/audio/speech_to_text
Headers: X-API-Key: your_key
Body: {"audio": "base64_encoded_audio"}

# Vision (Multimodal AI)
POST https://api.lab.vispark.in/model/text/vision
Headers: X-API-Key: your_key
Body: {"size": "small/medium/large", "content": [...], "system_message": "..."}
```

---

## ğŸ“ Support & Contributing

For questions about the benchmarking framework or to contribute improvements:

- **Documentation:** Each benchmark folder contains detailed README files
- **Issues:** Report bugs or request features
- **Contributing:** Pull requests welcome for new benchmarks or improvements

---

## ğŸ“„ License & Attribution

This benchmarking framework is designed for evaluating speech-to-speech AI models using industry-standard methodologies and datasets.

**Last Updated:** August 30, 2025
**Framework Version:** v1.0
**Tested Models:** 4 major S2S models

---

*This comprehensive benchmarking framework provides the most thorough evaluation of speech-to-speech AI models available, covering all critical aspects from technical performance to ethical alignment.*
